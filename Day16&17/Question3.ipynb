{"cells":[{"cell_type":"code","execution_count":43,"id":"1c111496","metadata":{},"outputs":[],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._\n","import spark.implicits._\n","import org.apache.spark.rdd.RDD\n","import scala.util.parsing.json.JSON\n","import org.apache.spark.sql.DataFrame"]},{"cell_type":"markdown","id":"80b6293b","metadata":{},"source":["## Case Study 3: Handling Incomplete Metadata\n","\n","Objective: Enrich incomplete movie metadata using additional JSON files.\n","\n","Scenario: Movielens metadata (e.g., movies.csv) is missing releaseYear for some movies. Supplementary metadata in JSON format is available for enrichment.\n","Steps:\n","\n","Ingestion:\n","\n","Load movies.csv from GCP Cloud Storage as a DataFrame.\n","Load metadata.json from GCP Cloud Storage into an RDD for custom parsing.\n","Transformation:\n","\n","Use RDD operations to parse the JSON file and extract movieId and releaseYear.\n","Perform an RDD join with the movies DataFrame to fill in missing releaseYear.\n","Validation:\n","\n","Convert the enriched RDD back into a DataFrame.\n","Validate that all movies have a releaseYear field.\n","Storage:\n","\n","Save the enriched DataFrame in Parquet format in HDFS."]},{"cell_type":"code","execution_count":42,"id":"bff6bdf9","metadata":{},"outputs":[{"data":{"text/plain":["conf = org.apache.spark.SparkConf@3a0c030\n","sc = org.apache.spark.SparkContext@61790a83\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.SparkContext@61790a83"]},"execution_count":42,"metadata":{},"output_type":"execute_result"}],"source":["val conf = new SparkConf()\n","      .setAppName(\"Partitioning Impact on Performance\")\n","      .setMaster(\"yarn\")\n","\n","val sc = new SparkContext(conf)"]},{"cell_type":"code","execution_count":44,"id":"b6addc35","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Metadata written successfully to gs://scala_assgn_bucket/ml-32m/metadata.json!\n"]},{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@ec536f0\n","bucketName = scala_assgn_bucket\n","moviesPath = gs://scala_assgn_bucket/ml-32m/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","extractYear = SparkUserDefinedFunction($Lambda$6569/0x0000000802385840@4fd4588f,StringType,List(Some(class[value[0]: string])),Some(class[value[0]: string]),None,true,true)\n","metadataDF = [movieId: string, title: string ... 1 more field]\n","outputPath = gs://scala_assgn_bucket/ml-32m/metadata.json\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["gs://scala_assgn_bucket/ml-32m/metadata.json"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.functions._\n","import scala.util.Random\n","\n","// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Generate Metadata JSON\")\n","  .getOrCreate()\n","\n","// Step 2: Load movies.csv as a DataFrame\n","val bucketName = \"scala_assgn_bucket\"\n","val moviesPath = s\"gs://$bucketName/ml-32m/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 3: Extract releaseYear from title or assign a random year\n","val extractYear = udf((title: String) => {\n","  val yearPattern = \"\\\\((\\\\d{4})\\\\)\".r\n","  yearPattern.findFirstMatchIn(title).map(_.group(1)).getOrElse {\n","    (1980 + Random.nextInt(2024 - 1980 + 1)).toString\n","  }\n","})\n","\n","// Generate metadata DataFrame\n","val metadataDF = moviesDF\n","  .select(\"movieId\", \"title\")\n","  .withColumn(\"releaseYear\", extractYear(col(\"title\")))\n","\n","// Step 4: Write the DataFrame as a single JSON file to GCS\n","val outputPath = s\"gs://$bucketName/ml-32m/metadata.json\"\n","\n","metadataDF.coalesce(1) // Ensures a single output file\n","  .write\n","  .mode(\"overwrite\")\n","  .json(outputPath)\n","\n","println(s\"Metadata written successfully to $outputPath!\")"]},{"cell_type":"code","execution_count":22,"id":"d02e4b8f","metadata":{},"outputs":[{"data":{"text/plain":["moviesDF = [movieId: int, title: string ... 1 more field]\n","metadataPath = gs://scala_assgn_bucket/ml-32m/metadata.json\n","metadataRDD = gs://scala_assgn_bucket/ml-32m/metadata.json MapPartitionsRDD[121] at textFile at <console>:61\n","parsedMetadataRDD = MapPartitionsRDD[122] at map at <console>:64\n","metadataFromJsonDF = [movieId: int, releaseYear: int]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["<console>:66: warning: non-variable type argument String in type pattern scala.collection.immutable.Map[String,Any] (the underlying of Map[String,Any]) is unchecked since it is eliminated by erasure\n","           case Some(json: Map[String, Any]) =>\n","                           ^\n","<console>:65: warning: match may not be exhaustive.\n","It would fail on the following inputs: None, Some((x: Any forSome x not in scala.collection.immutable.Map[?,?]))\n","         JSON.parseFull(line) match {\n","                       ^\n","warning: one deprecation (since 1.0.6); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: int, releaseYear: int]"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["// Step 1: Load `movies.csv` as DataFrame\n","val moviesDF = spark.read\n","  .option(\"header\", \"true\") // CSV has header\n","  .option(\"inferSchema\", \"true\") // Infer data types\n","  .csv(moviesPath)\n","\n","// Step 2: Load `metadata.json` into RDD\n","val metadataPath = s\"gs://$bucketName/ml-32m/metadata.json\"\n","val metadataRDD: RDD[String] = spark.sparkContext.textFile(metadataPath)\n","\n","// Parse JSON to extract `movieId` and `releaseYear`\n","val parsedMetadataRDD: RDD[(Int, Int)] = metadataRDD.map { line =>\n","  JSON.parseFull(line) match {\n","    case Some(json: Map[String, Any]) =>\n","      val movieId = json.get(\"movieId\").map(_.toString.toInt)\n","      val releaseYear = json.get(\"releaseYear\").map(_.toString.toInt)\n","      (movieId.get, releaseYear.get)\n","  }\n","}\n","\n","// Convert metadata RDD to DataFrame\n","val metadataFromJsonDF = parsedMetadataRDD.toDF(\"movieId\", \"releaseYear\")"]},{"cell_type":"code","execution_count":23,"id":"13949380","metadata":{},"outputs":[{"data":{"text/plain":["moviesRDD = MapPartitionsRDD[128] at map at <console>:49\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MapPartitionsRDD[128] at map at <console>:49"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["// Step 3: Convert movies DataFrame to RDD for join\n","val moviesRDD: RDD[(Int, (String, String))] = moviesDF.rdd.map(row => {\n","  val movieId = row.getAs[Int](\"movieId\")\n","  val title = row.getAs[String](\"title\")\n","  val genres = row.getAs[String](\"genres\")\n","  (movieId, (title, genres))\n","})"]},{"cell_type":"code","execution_count":37,"id":"e25c0848","metadata":{},"outputs":[{"data":{"text/plain":["enrichedRDD = MapPartitionsRDD[150] at mapValues at <console>:61\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MapPartitionsRDD[150] at mapValues at <console>:61"]},"execution_count":37,"metadata":{},"output_type":"execute_result"}],"source":["// Perform RDD join to enrich `releaseYear` where missing\n","val enrichedRDD: RDD[(Int, (String, String))] = moviesRDD.leftOuterJoin(parsedMetadataRDD).mapValues { \n","    case ((title, genres), releaseYear) =>\n","        var enrichedTitled = title\n","        if (!title.matches(\".*\\\\(\\\\d{4}\\\\)$\")) {\n","            enrichedTitled = s\"$title (${releaseYear.get})\"\n","        }\n","        (enrichedTitled, genres)\n","}"]},{"cell_type":"code","execution_count":39,"id":"4e7d19c3","metadata":{},"outputs":[{"data":{"text/plain":["enrichedMoviesDF = [movieId: int, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: int, title: string ... 1 more field]"]},"execution_count":39,"metadata":{},"output_type":"execute_result"}],"source":["// Step 4: Convert enriched RDD back to DataFrame\n","val enrichedMoviesDF: DataFrame = enrichedRDD.map {\n","  case (movieId, (title, genres)) =>\n","    (movieId, title, genres)\n","}.toDF(\"movieId\", \"title\", \"genres\")"]},{"cell_type":"code","execution_count":40,"id":"1f326f9d","metadata":{},"outputs":[{"data":{"text/plain":["missingYearsCount = 0\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["All movies have a releaseYear.\n"]},{"data":{"text/plain":["0"]},"execution_count":40,"metadata":{},"output_type":"execute_result"}],"source":["// Step 5: Validate all movies have `releaseYear`\n","val missingYearsCount = enrichedMoviesDF.filter(!col(\"title\").rlike(\"\\\\(\\\\d{4}\\\\)$\")).count()\n","if (missingYearsCount > 0) {\n","  println(s\"Warning: $missingYearsCount movies still missing releaseYear.\")\n","} else {\n","  println(\"All movies have a releaseYear.\")\n","}"]},{"cell_type":"code","execution_count":45,"id":"af4f44f3","metadata":{},"outputs":[{"ename":"org.apache.spark.SparkException","evalue":"Job aborted.","output_type":"error","traceback":["org.apache.spark.SparkException: Job aborted.","  at org.apache.spark.sql.errors.QueryExecutionErrors$.jobAbortedError(QueryExecutionErrors.scala:650)","  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:309)","  at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:186)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)","  at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)","  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:109)","  at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:169)","  at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:95)","  at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:779)","  at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)","  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)","  at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)","  at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:584)","  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:176)","  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:584)","  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:30)","  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)","  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)","  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)","  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:30)","  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:560)","  at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)","  at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)","  at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)","  at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:116)","  at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:860)","  at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:390)","  at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:363)","  at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)","  at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:793)","  ... 60 elided","Caused by: java.lang.IllegalStateException: SparkContext has been shutdown","  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2285)","  at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:257)","  ... 90 more"]}],"source":["// Step 6: Save the enriched DataFrame as Parquet in HDFS\n","val outputParquetPath = \"hdfs:///user/shraman_jana/enriched-movies.parquet\"\n","enrichedMoviesDF.write.mode(\"overwrite\").parquet(outputParquetPath)\n","\n","println(s\"Enriched movies data saved to $outputParquetPath\")\n","\n","// Stop Spark Session\n","spark.stop()"]},{"cell_type":"code","execution_count":46,"id":"280979a6","metadata":{},"outputs":[{"data":{"text/plain":["lastException = null\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["null"]},"execution_count":46,"metadata":{},"output_type":"execute_result"}],"source":["spark.stop()"]},{"cell_type":"code","execution_count":null,"id":"5cb55c47","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}