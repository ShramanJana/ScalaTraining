{"cells":[{"cell_type":"code","execution_count":2,"id":"5efa5915","metadata":{},"outputs":[],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._"]},{"cell_type":"markdown","id":"91b85efb","metadata":{},"source":["## Case Study 2: User Rating History Partitioning\n","\n","Objective: Partition the Movielens dataset by user for faster query processing.\n","\n","Scenario: Movielens user ratings data (CSV format) needs to be partitioned into separate folders for each user in HDFS.\n","Steps:\n","\n","Ingestion: Load the ratings.csv file as a DataFrame from GCP Cloud Storage.\n","\n","Transformation:\n","\n","Use a DataFrame to filter out invalid or incomplete records.\n","Convert the DataFrame into an RDD to dynamically create key-value pairs of userId and their corresponding ratings.\n","Partitioning:\n","\n","Use RDD transformations like groupByKey to partition ratings data by userId.\n","Write each user's data to a separate folder in HDFS using the saveAsTextFile method.\n","Verification:\n","\n","Validate that the HDFS structure follows the format /user-data/{userId}/ratings.csv.\n"]},{"cell_type":"code","execution_count":3,"id":"3320dc40","metadata":{},"outputs":[{"data":{"text/plain":["conf = org.apache.spark.SparkConf@777ffafe\n","sc = org.apache.spark.SparkContext@1fe8160\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.SparkContext@1fe8160"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["val conf = new SparkConf()\n","        .setAppName(\"User Rating History Partitioning\")\n","        .setMaster(\"yarn\")\n","val sc = new SparkContext(conf)"]},{"cell_type":"code","execution_count":4,"id":"40a6e13b","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+-------+------+---------+\n","|userId|movieId|rating|timestamp|\n","+------+-------+------+---------+\n","|     1|     17|   4.0|944249077|\n","|     1|     25|   1.0|944250228|\n","|     1|     29|   2.0|943230976|\n","|     1|     30|   5.0|944249077|\n","|     1|     32|   5.0|943228858|\n","|     1|     34|   2.0|943228491|\n","|     1|     36|   1.0|944249008|\n","|     1|     80|   5.0|944248943|\n","|     1|    110|   3.0|943231119|\n","|     1|    111|   5.0|944249008|\n","|     1|    161|   1.0|943231162|\n","|     1|    166|   5.0|943228442|\n","|     1|    176|   4.0|944079496|\n","|     1|    223|   3.0|944082810|\n","|     1|    232|   5.0|943228442|\n","|     1|    260|   5.0|943228696|\n","|     1|    302|   4.0|944253272|\n","|     1|    306|   5.0|944248888|\n","|     1|    307|   5.0|944253207|\n","|     1|    322|   4.0|944053801|\n","+------+-------+------+---------+\n","only showing top 20 rows\n","\n"]},{"data":{"text/plain":["bucketName = scala_assgn_bucket\n","ratingsPath = gs://scala_assgn_bucket/ml-32m/ratings.csv\n","ratingsDF = [userId: string, movieId: string ... 2 more fields]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[userId: string, movieId: string ... 2 more fields]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["val bucketName = \"scala_assgn_bucket\"\n","// Load ratings.csv from GCP Cloud Storage\n","val ratingsPath = s\"gs://$bucketName/ml-32m/ratings.csv\"\n","val ratingsDF = spark.read.option(\"header\", \"true\").csv(ratingsPath)\n","\n","ratingsDF.show()"]},{"cell_type":"code","execution_count":5,"id":"f5da8ba6","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["(31448,(48516,4.5))\n","(31448,(48780,4.0))\n","(31448,(49272,4.5))\n","(31448,(54286,4.5))\n","(31448,(55820,5.0))\n"]},{"data":{"text/plain":["validRatingsDF = [userId: string, movieId: string ... 2 more fields]\n","trimmedDF = [userId: string, movieId: string ... 2 more fields]\n","ratingsRDD = MapPartitionsRDD[22] at map at <console>:47\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MapPartitionsRDD[22] at map at <console>:47"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["// Filter out invalid records where rating is null or missing\n","val validRatingsDF = ratingsDF.filter(col(\"userId\").isNotNull && col(\"userId\").isNotNull && col(\"rating\").isNotNull)\n","\n","val trimmedDF = validRatingsDF.limit(100000)\n","// Convert DataFrame to RDD (for dynamic transformations)\n","val ratingsRDD = trimmedDF.rdd.map(row => {\n","  val userId = row.getAs[String](\"userId\")\n","  val movieId = row.getAs[String](\"movieId\")\n","  val rating = row.getAs[String](\"rating\").toDouble\n","  (userId, (movieId, rating))  // (userId, (movieId, rating))\n","})\n","\n","ratingsRDD.take(5).foreach(println)  // Inspect a few records"]},{"cell_type":"code","execution_count":6,"id":"e414676f","metadata":{},"outputs":[{"data":{"text/plain":["groupedByUserRDD = MapPartitionsRDD[24] at mapValues at <console>:40\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["MapPartitionsRDD[24] at mapValues at <console>:40"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["// Group ratings by userId\n","val groupedByUserRDD = ratingsRDD.groupByKey().mapValues(_.toList)"]},{"cell_type":"code","execution_count":8,"id":"e3d64617","metadata":{},"outputs":[{"data":{"text/plain":["outputPath = hdfs:///user/shraman_jana/user-data/Q2\n","first10Users = Array((273,List((1,4.0), (60,3.5), (260,4.5), (364,4.5), (519,1.5), (588,4.0), (595,4.5), (653,4.5), (780,3.5), (1015,3.5), (1196,3.5), (1197,4.5), (1200,1.5), (1210,4.0), (1566,2.5), (1580,3.5), (2006,4.5), (2571,4.0), (2628,4.0), (3114,4.0), (3793,4.0), (4016,5.0), (4306,4.0), (4886,4.0), (4896,4.0), (4973,4.5), (4993,5.0), (5218,4.5), (5349,3.5), (5433,4.0), (5444,4.5), (5459,3.5), (5618,3.5), (5952,4.5), (6333,4.0), (6365,2.5), (6373,3.5), (6377,3.5), (6539,4.5), (6754,2.0), (7153,4.5), (7373,3.0), (8360,4.5), (8368,4.0), (8464,2.5), (8636,...\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["Array((273,List((1,4.0), (60,3.5), (260,4.5), (364,4.5), (519,1.5), (588,4.0), (595,4.5), (653,4.5), (780,3.5), (1015,3.5), (1196,3.5), (1197,4.5), (1200,1.5), (1210,4.0), (1566,2.5), (1580,3.5), (2006,4.5), (2571,4.0), (2628,4.0), (3114,4.0), (3793,4.0), (4016,5.0), (4306,4.0), (4886,4.0), (4896,4.0), (4973,4.5), (4993,5.0), (5218,4.5), (5349,3.5), (5433,4.0), (5444,4.5), (5459,3.5), (5618,3.5), (5952,4.5), (6333,4.0), (6365,2.5), (6373,3.5), (6377,3.5), (6539,4.5), (6754,2.0), (7153,4.5), (7373,3.0), (8360,4.5), (8368,4.0), (8464,2.5), (8636,..."]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["import org.apache.spark.rdd.RDD\n","import org.apache.hadoop.fs.{FileSystem, Path}\n","import java.io.{BufferedWriter, OutputStreamWriter}\n","\n","val outputPath = \"hdfs:///user/shraman_jana/user-data/Q2\"\n","    val first10Users = groupedByUserRDD.take(10)\n","first10Users.foreach { case (userId, ratingsList) =>\n","  val userFolderPath = s\"${outputPath}/${userId}/ratings.csv\"\n","  val path = new Path(userFolderPath)\n","  \n","  val fs = FileSystem.get(new java.net.URI(\"hdfs:///\"), new org.apache.hadoop.conf.Configuration())\n","  \n","  if (!fs.exists(path.getParent)) {\n","    fs.mkdirs(path.getParent)\n","  }\n","\n","  val ratingsText = ratingsList.map { case (movieId, rating) =>\n","    s\"${movieId}, ${rating}\"\n","  }.mkString(\"\\n\")\n","\n","  val outputStream = fs.create(path)\n","  val writer = new BufferedWriter(new OutputStreamWriter(outputStream))\n","\n","  writer.write(ratingsText)\n","\n","  writer.close()\n","  outputStream.close()\n","}"]},{"cell_type":"code","execution_count":9,"id":"a9f96c68","metadata":{},"outputs":[],"source":["sc.stop()"]},{"cell_type":"code","execution_count":null,"id":"51b86336","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}