{"cells":[{"cell_type":"code","execution_count":13,"id":"78a7b86b","metadata":{},"outputs":[],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.DataFrame"]},{"cell_type":"markdown","id":"a182abd2","metadata":{},"source":["## Case Study 4: Duplicate Record Removal Pipeline\n","\n","Objective: Identify and remove duplicate movie records based on movieId and title, saving clean data in Avro format.\n","\n","Scenario: The movies.csv file in HDFS contains duplicate records that need to be cleaned.\n","Steps:\n","\n","Ingestion: Load movies.csv into a Spark DataFrame from HDFS.\n","\n","Transformation:\n","\n","Use DataFrames to identify duplicates based on movieId and title.\n","Convert the DataFrame to an RDD to perform custom filtering operations using distinct() on a composite key (movieId, title).\n","Validation:\n","\n","Count the number of duplicates removed by comparing the record counts before and after transformation.\n","Storage:\n","\n","Save the cleaned data as Avro files in GCP Cloud Storage."]},{"cell_type":"code","execution_count":2,"id":"0f4d02cc","metadata":{},"outputs":[{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@477e84e0\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.sql.SparkSession@477e84e0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["// Step 1: Initialize SparkSession\n","val spark = SparkSession.builder()\n","  .appName(\"Transfer File from GCS to HDFS\")\n","  .getOrCreate()"]},{"cell_type":"code","execution_count":5,"id":"2aadf7f5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File transferred from gs://scala_assgn_bucket/ml-32m/movies.csv to hdfs:///user/shraman_jana/Q4/movies.csv successfully!\n"]},{"data":{"text/plain":["bucketName = scala_assgn_bucket\n","moviesPath = gs://scala_assgn_bucket/ml-32m/movies.csv\n","hdfsPath = hdfs:///user/shraman_jana/Q4/movies.csv\n","data = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["// Step 2: Define the GCS path and HDFS path\n","val bucketName = \"scala_assgn_bucket\"\n","val moviesPath = s\"gs://$bucketName/ml-32m/movies.csv\"\n","val hdfsPath = \"hdfs:///user/shraman_jana/Q4/movies.csv\"\n","\n","// Step 3: Read the file from GCS\n","val data = spark.read\n","  .option(\"header\", \"true\")  // Read the header from the CSV file\n","  .csv(moviesPath)\n","\n","// Step 4: Write the file to HDFS with headers\n","data.write\n","  .option(\"header\", \"true\")  // Include the header in the output\n","  .mode(\"overwrite\")         // Overwrite if the file already exists\n","  .csv(hdfsPath)\n","\n","println(s\"File transferred from $moviesPath to $hdfsPath successfully!\")"]},{"cell_type":"code","execution_count":6,"id":"6a036d1d","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000 duplicates inserted and file updated successfully!\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/shraman_jana/Q4/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","duplicateMoviesDF = [movieId: string, title: string ... 1 more field]\n","moviesWithDuplicatesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["//Adding duplicates\n","\n","// Step 1: Read the existing movies.csv from HDFS\n","val moviesPath = \"hdfs:///user/shraman_jana/Q4/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 2: Add 1000 duplicates by appending the same DataFrame multiple times\n","val duplicateMoviesDF = moviesDF.limit(1000) // Take 1000 rows to duplicate\n","val moviesWithDuplicatesDF = moviesDF.union(duplicateMoviesDF) // Append duplicates\n","\n","// Step 3: Write the updated DataFrame with duplicates back to the same HDFS path\n","moviesWithDuplicatesDF.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\") // Overwrite the existing file\n","  .csv(\"hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\")\n","\n","println(\"1000 duplicates inserted and file updated successfully!\")"]},{"cell_type":"code","execution_count":10,"id":"54b28d53","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original record count: 88585\n","Deduplicated record count: 87585\n","Duplicates removed: 1000\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","moviesRDD = MapPartitionsRDD[46] at map at <console>:56\n","uniqueMoviesRDD = ShuffledRDD[47] at reduceByKey at <console>:64\n","cleanedMoviesRDD = MapPartitionsRDD[48] at map at <console>:67\n","cleanedMoviesDF = [movieId: string, title: string ... 1 more field]\n","originalCount = 88585\n","deduplicatedCount = 87585\n","duplicatesRemoved = 1000\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1000"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["// Step 2: Load movies.csv into a DataFrame from HDFS\n","val moviesPath = \"hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 3: Convert to RDD with composite key (movieId, title)\n","val moviesRDD = moviesDF.rdd.map(row => {\n","  val movieId = row.getString(row.fieldIndex(\"movieId\"))\n","  val title = row.getString(row.fieldIndex(\"title\"))\n","  val genres = row.getString(row.fieldIndex(\"genres\"))\n","  ((movieId, title), genres) // Key: (movieId, title), Value: genres\n","})\n","\n","// Combine genres for duplicate keys\n","val uniqueMoviesRDD = moviesRDD.reduceByKey((genres1, genres2) => s\"$genres1|$genres2\")\n","\n","// Transform back to (movieId, title, genres) format\n","val cleanedMoviesRDD = uniqueMoviesRDD.map {\n","  case ((movieId, title), combinedGenres) => (movieId, title, combinedGenres)\n","}\n","\n","val cleanedMoviesDF = cleanedMoviesRDD.toDF(\"movieId\", \"title\", \"genres\")\n","\n","// Step 4: Validation\n","val originalCount = moviesDF.count()\n","val deduplicatedCount = cleanedMoviesDF.count()\n","val duplicatesRemoved = originalCount - deduplicatedCount\n","println(s\"Original record count: $originalCount\")\n","println(s\"Deduplicated record count: $deduplicatedCount\")\n","println(s\"Duplicates removed: $duplicatesRemoved\")"]},{"cell_type":"code","execution_count":12,"id":"2bf902f1","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaned movies data saved successfully in Avro format.\n"]},{"data":{"text/plain":["lastException = null\n","outputPath = gs://scala_assgn_bucket/Day16_17/Q4/cleaned_movies.avro\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["gs://scala_assgn_bucket/Day16_17/Q4/cleaned_movies.avro"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["// Step 6: Save the cleaned data as Avro files in GCP Cloud Storage\n","val outputPath = s\"gs://$bucketName/Day16_17/Q4/cleaned_movies.avro\"\n","cleanedMoviesDF.write\n","  .format(\"avro\")\n","  .mode(\"overwrite\")\n","  .save(outputPath)\n","\n","println(\"Cleaned movies data saved successfully in Avro format.\")"]},{"cell_type":"code","execution_count":17,"id":"51a05da0","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial movies duplicate count:\n","1000\n","Cleaned movies dataset duplicate count:\n"]},{"data":{"text/plain":["countDuplicates: (inputDF: org.apache.spark.sql.DataFrame)Long\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["0\n"]}],"source":["//Method to count duplicates\n","\n","def countDuplicates(inputDF: DataFrame): Long = {\n","    val groupedDF = inputDF\n","    .groupBy(\"movieId\", \"title\")\n","    .count()\n","\n","    val duplicateGroupsDF = groupedDF.filter(col(\"count\") > 1)\n","\n","    if (duplicateGroupsDF.head(1).isEmpty) {\n","        return 0L\n","    }\n","\n","    val duplicateCount = duplicateGroupsDF\n","    .select(expr(\"sum(count - 1)\")) \n","    .collect()(0)(0)\n","\n","    duplicateCount.toString.toLong\n","}\n","\n","println(\"Initial movies duplicate count:\")\n","println(countDuplicates(moviesDF))\n","println(\"Cleaned movies dataset duplicate count:\")\n","println(countDuplicates(cleanedMoviesDF))"]},{"cell_type":"code","execution_count":null,"id":"aa3c4fd2","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}