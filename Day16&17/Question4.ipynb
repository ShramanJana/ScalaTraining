{"cells":[{"cell_type":"code","execution_count":13,"id":"74a4200d","metadata":{},"outputs":[],"source":["import org.apache.spark.{SparkConf, SparkContext}\n","import org.apache.spark.sql.functions._\n","import org.apache.spark.sql.SparkSession\n","import org.apache.spark.sql.DataFrame"]},{"cell_type":"markdown","id":"8fa6d643","metadata":{},"source":["## Case Study 4: Duplicate Record Removal Pipeline\n","\n","Objective: Identify and remove duplicate movie records based on movieId and title, saving clean data in Avro format.\n","\n","Scenario: The movies.csv file in HDFS contains duplicate records that need to be cleaned.\n","Steps:\n","\n","Ingestion: Load movies.csv into a Spark DataFrame from HDFS.\n","\n","Transformation:\n","\n","Use DataFrames to identify duplicates based on movieId and title.\n","Convert the DataFrame to an RDD to perform custom filtering operations using distinct() on a composite key (movieId, title).\n","Validation:\n","\n","Count the number of duplicates removed by comparing the record counts before and after transformation.\n","Storage:\n","\n","Save the cleaned data as Avro files in GCP Cloud Storage."]},{"cell_type":"code","execution_count":2,"id":"e4be3a01","metadata":{},"outputs":[{"data":{"text/plain":["spark = org.apache.spark.sql.SparkSession@477e84e0\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["org.apache.spark.sql.SparkSession@477e84e0"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["// Step 1: Initialize SparkSession\n","val conf = new SparkConf()\n","      .setAppName(\"Transfer File from GCS to HDFS\")\n","      .setMaster(\"yarn\")\n","\n","val sc = new SparkContext(conf)"]},{"cell_type":"code","execution_count":5,"id":"33474ae4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["File transferred from gs://scala_assgn_bucket/ml-32m/movies.csv to hdfs:///user/shraman_jana/Q4/movies.csv successfully!\n"]},{"data":{"text/plain":["bucketName = scala_assgn_bucket\n","moviesPath = gs://scala_assgn_bucket/ml-32m/movies.csv\n","hdfsPath = hdfs:///user/shraman_jana/Q4/movies.csv\n","data = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["// Step 2: Define the GCS path and HDFS path\n","val bucketName = \"scala_assgn_bucket\"\n","val moviesPath = s\"gs://$bucketName/ml-32m/movies.csv\"\n","val hdfsPath = \"hdfs:///user/shraman_jana/Q4/movies.csv\"\n","\n","// Step 3: Read the file from GCS\n","val data = spark.read\n","  .option(\"header\", \"true\")  // Read the header from the CSV file\n","  .csv(moviesPath)\n","\n","// Step 4: Write the file to HDFS with headers\n","data.write\n","  .option(\"header\", \"true\")  // Include the header in the output\n","  .mode(\"overwrite\")         // Overwrite if the file already exists\n","  .csv(hdfsPath)\n","\n","println(s\"File transferred from $moviesPath to $hdfsPath successfully!\")"]},{"cell_type":"code","execution_count":6,"id":"9de358a4","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1000 duplicates inserted and file updated successfully!\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/shraman_jana/Q4/movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","duplicateMoviesDF = [movieId: string, title: string ... 1 more field]\n","moviesWithDuplicatesDF = [movieId: string, title: string ... 1 more field]\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["[movieId: string, title: string ... 1 more field]"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["//Adding duplicates\n","\n","// Step 1: Read the existing movies.csv from HDFS\n","val moviesPath = \"hdfs:///user/shraman_jana/Q4/movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 2: Add 1000 duplicates by appending the same DataFrame multiple times\n","val sampleMoviesDF = moviesDF.limit(1000) // Take 1000 rows to duplicate\n","val duplicateMoviesDF = moviesDF.union(sampleMoviesDF) // Append duplicates\n","\n","// Step 3: Write the updated DataFrame with duplicates back to the same HDFS path\n","duplicateMoviesDF.write\n","  .option(\"header\", \"true\")\n","  .mode(\"overwrite\") // Overwrite the existing file\n","  .csv(\"hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\")\n","\n","println(\"1000 duplicates inserted and file updated successfully!\")"]},{"cell_type":"code","execution_count":18,"id":"ebdcfa4f","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Original record count: 88585\n","Deduplicated record count: 87585\n","Duplicates removed: 1000\n"]},{"data":{"text/plain":["moviesPath = hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\n","moviesDF = [movieId: string, title: string ... 1 more field]\n","cleanedMoviesDF = [movieId: string, title: string ... 1 more field]\n","originalCount = 88585\n","deduplicatedCount = 87585\n","duplicatesRemoved = 1000\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1000"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["// Step 2: Load movies.csv into a DataFrame from HDFS\n","val moviesPath = \"hdfs:///user/shraman_jana/Q4/duplicated_movies.csv\"\n","val moviesDF = spark.read.option(\"header\", \"true\").csv(moviesPath)\n","\n","// Step 3: Convert to RDD with composite key (movieId, title) and Remove duplicate records\n","val cleanedMoviesDF = moviesDF.rdd.map(row => {\n","  val movieId = row.getString(row.fieldIndex(\"movieId\"))\n","  val title = row.getString(row.fieldIndex(\"title\"))\n","  val genres = row.getString(row.fieldIndex(\"genres\"))\n","  ((movieId, title), genres) // Key: (movieId, title), Value: genres\n","}).reduceByKey((genres1, genres2) => s\"$genres1|$genres2\").map {\n","  case ((movieId, title), combinedGenres) => (movieId, title, combinedGenres)\n","}.toDF(\"movieId\", \"title\", \"genres\")\n","\n","// Step 4: Validation\n","val originalCount = moviesDF.count()\n","val deduplicatedCount = cleanedMoviesDF.count()\n","val duplicatesRemoved = originalCount - deduplicatedCount\n","println(s\"Original record count: $originalCount\")\n","println(s\"Deduplicated record count: $deduplicatedCount\")\n","println(s\"Duplicates removed: $duplicatesRemoved\")"]},{"cell_type":"code","execution_count":12,"id":"4dfd7c6c","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Cleaned movies data saved successfully in Avro format.\n"]},{"data":{"text/plain":["lastException = null\n","outputPath = gs://scala_assgn_bucket/Day16_17/Q4/cleaned_movies.avro\n"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["gs://scala_assgn_bucket/Day16_17/Q4/cleaned_movies.avro"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["// Step 6: Save the cleaned data as Avro files in GCP Cloud Storage\n","val outputPath = s\"gs://$bucketName/Day16_17/Q4/cleaned_movies.avro\"\n","cleanedMoviesDF.write\n","  .format(\"avro\")\n","  .mode(\"overwrite\")\n","  .save(outputPath)\n","\n","println(\"Cleaned movies data saved successfully in Avro format.\")"]},{"cell_type":"code","execution_count":21,"id":"29abd239","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Initial movies duplicate count:\n","Record count after removing duplicates: 87585\n","Duplicates removed: 1000\n","1000\n","Cleaned movies dataset duplicate count:\n"]},{"data":{"text/plain":["countDuplicates: (inputDF: org.apache.spark.sql.DataFrame)Long\n"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Record count after removing duplicates: 87585\n","Duplicates removed: 0\n","0\n"]}],"source":["//Method to count duplicates\n","\n","def countDuplicates(inputDF: DataFrame): Long = {\n","    val deduplicatedDf = inputDF.dropDuplicates(\"movieId\", \"title\")\n","    \n","    val duplicateCount = inputDF.count() - deduplicatedDf.count()\n","    println(s\"Record count after removing duplicates: ${deduplicatedDf.count()}\")\n","    println(s\"Duplicates removed: ${duplicateCount}\")\n","    duplicateCount\n","}\n","\n","println(\"Initial movies duplicate count:\")\n","println(countDuplicates(moviesDF))\n","println(\"Cleaned movies dataset duplicate count:\")\n","println(countDuplicates(cleanedMoviesDF))"]},{"cell_type":"code","execution_count":22,"id":"f4cd0214","metadata":{},"outputs":[],"source":["sc.stop()"]},{"cell_type":"code","execution_count":null,"id":"f0934c93","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Apache Toree - Scala","language":"scala","name":"apache_toree_scala"},"language_info":{"codemirror_mode":"text/x-scala","file_extension":".scala","mimetype":"text/x-scala","name":"scala","pygments_lexer":"scala","version":"2.12.15"}},"nbformat":4,"nbformat_minor":5}